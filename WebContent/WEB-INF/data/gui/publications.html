<b><h1><a class='downloadText' href='http://svn.aksw.org/papers/2012/EKAW_BOA_EnDe_ML/public.pdf'>Extracting Multilingual Natural-Language Patterns for RDF Predicates</a></h1></b><br/>
Most knowledge sources on the Data Web were extracted from structured
 or semi-structured data. Thus, they encompass solely a small fraction 
 of the information available on the document-oriented Web. In this paper, 
 we present BOA, a bootstrapping strategy for ex- tracting RDF from text. 
 The idea behind BOA is to extract natural- language patterns that represent 
 predicates found on the Data Web from unstructured data by using background 
 knowledge from the Data Web. These patterns are then used to extract 
 instance knowledge from natural-language text. This knowledge is finally 
 fed back into the Data Web, therewith closing the loop. The approach followed 
 by BOA is quasi independent of the language in which the corpus is written. 
 We demonstrate our approach by applying it to four different corpora 
 and two different languages. We evaluate BOA on these data sets using 
 DBpedia as background knowledge. Our results show that we can extract several 
 thousand new facts in one iteration with very high accuracy.<br/><br/>

<i>@INPROCEEDINGS{Gerber2012, <br/>
&nbsp;&nbsp;author = {{Daniel Gerber and Axel-Cyrille Ngonga Ngomo}},<br/>
&nbsp;&nbsp;title = {Extracting Multilingual Natural-Language Patterns for RDF Predicates},<br/>
&nbsp;&nbsp;booktitle = {18th International Conference on Knowledge Engineering and Knowledge Management, Galway},<br/>
&nbsp;&nbsp;year = {2012}<br/>
}</i>

<b><h1><a class='downloadText' href='http://svn.aksw.org/papers/2012/ISWC_FactChecker/public.pdf'>Defacto - Deep Fact Validation</a></h1></b><br/>
One of the main tasks when creating and maintaining knowledge bases is 
to validate facts and provide sources for them in order to ensure correctness 
and traceability of the provided knowledge. So far, this task is often addressed 
by human curators in a three-step process: issuing appropriate keyword queries for 
the statement to check using standard search engines, retrieving potentially relevant 
documents and screening those documents for relevant content. The drawbacks of this
process are manifold. Most importantly, it is very time-consuming as the experts  
have to carry out several search processes and must often read several documents.  
In this article, we present DeFacto (Deep Fact Validation) - an algorithm for 
validating facts by finding trustworthy sources for it on the Web. DeFacto aims 
to provide an effective way of validating facts by supplying the user with relevant 
excerpts of webpages as well as useful additional information including a score 
for the confidence DeFacto has in the correctness of the input fact.

<i>@INPROCEEDINGS{LEH+12a, <br/>
  author = {Lehmann, Jens and Gerber, Daniel and Morsey, Mohamed and {Ngonga Ngomo}, Axel-Cyrille},<br/>
  booktitle = {11th International Semantic Web Conference},<br/>
  title = {DeFacto - Deep Fact Validation},<br/>
  year = 2012<br/>
}</i>


<b><h1><a class='downloadText' href='http://svn.aksw.org/papers/2012/ESWC_BOA-ML-PR/public.pdf'>BOA - Bootstrapping Linked Data:</a></h1></b><br/>
Most knowledge sources on the Data Web were extracted from structured
or semi-structured data. Thus, they encompass solely a small fraction
of the information available on the document-oriented Web. In this
paper, we present BOA, an iterative bootstrapping strategy for extracting
RDF from unstructured data. The idea behind BOA is to use the Data
Web as background knowledge for the extraction of natural language
patterns that represent predicates found on the Data Web. These patterns
are used to extract instance knowledge from natural language text.
This knowledge is finally fed back into the Data Web, therewith closing
the loop. We evaluate our approach on two data sets using DBpedia
as background knowledge. Our results show that we can extract several
thousand new facts in one iteration with very high accuracy. Moreover,
we provide the first repository of natural language representations
of predicates found on the Data Web.<br/><br/>

<i>@INPROCEEDINGS{Gerber2011, <br/>
&nbsp;&nbsp;author = {{Daniel Gerber and Axel-Cyrille Ngonga Ngomo}},<br/>
&nbsp;&nbsp;title = {Bootstrapping the Linked Data Web},<br/>
&nbsp;&nbsp;booktitle = {1st Workshop on Web Scale Knowledge Extraction @ ISWC 2011},<br/>
&nbsp;&nbsp;year = {2011}<br/>
}</i>
                
<b><h1><a class='downloadText' href=''>Template-based question answering over RDF data:</a></h1></b><br/> 
As an increasing amount of RDF data is published as Linked Data, 
intuitive ways of accessing this data become more and more important. 
Question answering approaches have been proposed as a good compromise 
between intuitiveness and expressivity. Most question answering systems 
translate questions into triples which are matched against the RDF data 
to retrieve an answer, typically relying on some similarity metric. However, 
in many cases, triples do not represent a faithful representation of the semantic 
structure of the natural language question, with the result that more expressive 
queries can not be answered. To circumvent this problem, we present a novel 
approach that relies on a parse of the question to produce a SPARQL template 
that directly mirrors the internal structure of the question. This template 
is then instantiated using statistical entity identification and predicate 
detection. We show that this approach is competitive and discuss cases of 
questions that can be answered with our approach but not with competing approaches.<br/><br/>
                
<i>@INPROCEEDINGS{UNG12,<br/>
&nbsp;&nbsp;author = {{Christina Unger, Lorenz B&uuml;hmann, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Daniel Gerber and Philipp Cimiano}},<br/>
&nbsp;&nbsp;title = {SPARQL Template Based Question Answering},<br/>
&nbsp;&nbsp;booktitle = {Proceedings of the 21st International World Wide Web Conference, WWW2012, Lyon (France), April 16-20, 2012},<br/>
&nbsp;&nbsp;year = {2012}<br/>
}</i>